---
title: Deep Learning Specialization
date: 2021-02-07 18:02:57
category: coursera
draft: false

---

앤드류 응 선생님의 딥러닝 전문가 코세라 강의. 구글 부트캠프 수료 과정에서 중점적으로 들어야하는 강의이다. 2017년에 나온 강의기에 최신 트렌드를 담았다고 볼 수는 없으나 딥러닝 기초를 확립하기에 좋은 강의이다. 이미 머신러닝과 딥러닝 기초적인 내용은 개인적으로 충분하다고 오만하게 판단했지만, 첫주차 강의를 듣고 그 생각이 바뀌었다. 다른 사람들도 이 강의를 통해 확실히 딥러닝 AI 지식 기초를 다질 수 있을 것이라 확신한다.

코스는 총 5개로 구성되어 있으며, 수료하면서 요약했던 내용을 개인적인 복습 차원에서 요약 및 정리를 하려고 한다. 역전파 등 수식 과정은 이 강의 내용에서나 유튜브에서나 훨씬 더 잘 설명되어 있다고 판단되어, 이론 위주로 내용을 정리해보았다. 물론 필연적으로 수식이 등장하겠지만 깔끔하게 보여지기 위해 강의 노트를 옮겨왔다. 기본적인 annotation은 안다고 가정하였다.

### Course 1. Neural Networks and Deep Learning

딥러닝을 알기 위해서는 먼저 `Logistic Regression` 을 알아야 한다. `Logistic Regression` 은 어떤 x (features) Y를 예측하는데 사용되는 기본 선형 확률 모델이다. 강의에 나왔던 예시로 예를 들자면 **어떤 사진**이 주어졌을 때 **사진이 고양이인지, 강아지인지 판단하는 작업**을 의미한다. 고양이인지, 강아지인지에 대한 분류 정확도를 로지스틱 회귀 모델은 0에서 1 사이의 확률로 표현해준다. 

이를 수식으로 간단히 정리하자면 예측값인 $\hat{y}$은 다음과 같다.
$$\hat{y} = \sigma(W^Tx + b)$$

여기서 중요한 부분은 `Sigmoid`함수 부분인데, 아래 그림은 `Sigmoid`함수를 나타낸다. 어떠한 값을 받아도 0 ~ 1 사이의 값으로 변환해주는 것을 볼 수 있다.

![logistic regression 이미지 검색결과](https://miro.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png)


**즉, 로지스틱 회귀 모델은 0 ~ 1 사이의 확률값으로 어떤 예측에 대한 수치를 표현하는 것이 핵심이다.** 만약 받은 값 z가 매우 클 경우 그 값은 1에 가까울 것이고 매우 작을 경우 0에 가까워질 것이다.

---

여기서 로지스틱 회귀 모델로부터 나온 예측값을 $y$가 아닌 $\hat{y}$으로 표현했다. 이유는 이 수치는 어디까지나 **정답**이 아닌 **예측값**이기 때문이다. 분류 모델의 정확도를 평가 하기 위해 로지스틱 회귀 모델에서는 정답과 예측값 간의 차이를 산출한 뒤, 이를 최소화하는 것을 목적으로 한다.  회귀분석 모델에서 파라미터 W와 b를 훈련시키기 위해서는 먼
회귀분석 모델에서 W와 B 파라미터를 트레이닝 시키기 위해서는 먼저

비용함수를 정의해야 합니다.

로지스틱 회귀 분석에 사용할 수 있는 비용 함수에 대해 살펴보도록 하죠.

복습하자면, 이전 슬라이드에서 찾았던 내용인데요.

결과값 ŷ은 w의 시그모이드

transpose x 더하기 b 이고, Z의 시그모이드는 이렇게 정의됩니다.

모델에 대한 파라미터를 학습하기 위해

m개의 트레이닝 example로 이루어진 트레이닝 세트가 주어지고,

트레이닝세트에서 매개 변수 W와

B를 찾는 것이 자연스러운 것 같습니다.

여러분의 트레이닝세트에 대한 예상치는

ŷ (i) 이라고 표기하는데요

트레이닝세트에서 나온 ground truth label y_i값과 비슷할 것입니다.

위에 있는 공식에대해 조금더 상세히 알려드리자면,

앞서 ŷ은 위처럼 정의된다고 했었죠,

x 트레이닝샘플에 대해서 말이죠.

그리고 각각의 크레이닝 샘플에 대해서요. 밑에 이 아래첨자에

괄호를 사용해서 인덱싱하고 example들을 차별화하시켰습니다.

트레이닝 샘플 (i)의 예상치는 ŷ(i)이며,

기 값은 시그모이드 함수를 W transpose X값에 적용하여,

(i) 트레이닝 example 입력값 더하기 V를 통해 Z(i)도 정의할 수 있습니다.

Z(i)는 W transpose x (i) 플러스 b입니다.

이번 코스를 통해

우린 규칙화된 표기법을 쓸 것인데요,

위 첨자의 괄호 i는 데이터를 뜻합니다.

X 또는 Y 또는 Z 또는 i번째 트레이닝 example과 연관된

i번째 example인거죠.

이것이 위 첨자 i가 의미하는 것입니다.

자 그럼 이제 알고리즘이 얼마나 잘 작동하고 있는지를 알아내기 위해

사용할 수 있는 loss 함수 또는 오류함수를 보도록 하겠습니다.

한가지 방법은 loss를 알고리즘의 결과값이 ŷ이고 true label인 Y가 제곱 오류 또는

0.5제곱 오류가 되게하는 것입니다.

이렇게 할 수 있는데요,

로지스틱 회귀분석법에서는 사람들이 이렇게 잘 하지 않습니다.

파라미터를 배우면서 알게되면,

최적한 문제에서 나중에 배우겠지만 비볼록하게 됩니다.

결과적으로 복수 국부 최적의 optimization 문제를 갖게 됩니다.

그렇게해서 gradient descent가 전역 최적값을 못 찾을 수 있습니다.

방금 이야기한 것들이 잘 이해가 안 가셨다고 하면

너무 걱정하지 마십시요. 추후 강의에서 더 자세히 다루도록 하겠습니다.

지금 이해햐셔야할 직관적인 부분은

L이라는 loss 함수는 true label y를 갖는 경우,

얼마나 정확히 ŷ 결과값을 산출하는지 정의할 때 사용합니다.

제곱 오류 값이 오히려 더 합리적인 방법이라고 생각할 수 있지만

gradient descent가 잘 안나온다는 단점이 있습니다.

그러므로 로지스틱 회귀분석법에서는,

여러가지 제곱 오류와 비슷한 역할을하는 loss함수를 정의해서

볼록한 최적화 문제를 주도록 할 것입니다.

그렇게되면 나중에 비디오 강의를 통해 보시겠지만, 최적화를 하기가 훨씬 더 쉬워집니다.

로지스틱 회귀분석법에서 사용하는 것은

여기 위에 있는 loss함수입니다.

여기서는 마이너스 y 로그 ŷ 더하기 1, 이 선은 y log

이 선은 ŷ 입니다.

직관적으로 이 loss 함수가 왜 말이 되는지 설명해드리겠습니다.

아셔야 할 것은,

제곱오류를 사용하는 경우, 이 값이 최대한 작아야 좋습니다.

regression loss함수를 이용하면,

이 것 또한 작은 값일 수록 좋죠.

이 것이 왜 말이 되는지,

2가지의 케이스를 통해서 보도록 하겠습니다.

첫번째 경우에는

Y가 1, 그리소

loss 함수 ŷ 컴마 y는 이 마이너스 부호를 쓸 수 있도록 해주죠.

그러므로 마이너스 로그 ŷ

만약 y가 1인 경우 말이죠. 그 이유는 y가 1이면,

두번째 항인 1-Y 는 0이 됩니다.

그럼 y가 1이면, 마이너스 로그 ŷ은 큰 값이되게 하는게 좋습니다.

그 뜻은 즉 log ŷ을 큰 값으로 만드는게 좋은것 인데요

최대한 크게 만드는 것이 좋기 때문에 ŷ이 큰 값을 갖게 하는 것이 좋겠죠.

그렇지만 ŷ은 아시다시피,

시그모이드 함수이기 때문에 1보다 큰 값을 가질 수 없습니다.

즉, y가 1인 경우

ŷ 값이 최대한 큰 값을 갖도록 하는 것이 좋다는 것을 알게 됩니다.

그렇지만 이 값은 절대로 1보다 큰 값이 될 수 없기 때문에, ŷ을 1과 최대한 가깝게 하는 것이 목표라고 해석할 수도 있습니다.

2번째 경우는 y가 0인 경우인데요,

만약 y가 0인 경우, loss 함수의 첫번째 항이 0이 됩니다.

y 가 0, 그러면 2번째 항이 loss함수를 정의하게 되죠.

이렇게 되면 loss는 마이너스 로그 1 마이너스 ŷ이 됩니다.

여러분의 러닝 단계에서 loss 함수를 작게 만들려고 한다면,

로그 1 마이너스 ŷ이 최대한 값을 갖길 원한다는 뜻입니다.

그리고 여기가 마이너스 부호이기 때문에

이 작은 이유 하나로

해당 loss 함수가 ŷ의 값을 최대한 작은 값으로 하려고 하는 것을 알 수 있습니다.

또, ŷ이 0에서 1사이 값을 가져야 하기 때문에

만약 y가 0이라면

loss 함수는 파라미터가 일을해서 ŷ이 최대한 0에 가까운 값이되도록 할 것입니다.

Rafidah's effect가 작용하는 여러가지 함수가 있는데요,

만약 y가 1인 경우, ŷ을 최대한 크게하고, Y가 0인 경우 ŷ을 작게하는 것입니다.

여기 초록색으로 된 부분을

아주 편안하게 비용함수에 대해 정의했는데요,

이 부분은 선택적인 비디오 강의 시청을 통해 조금 더

공식적인 배경을 설명하겠습니다. 왜 로지스틱 회귀분석법에서 이런 형식의 loss 함수를 사용하는지 말이죠.

마지막으로, loss 함수는 single training example을 바탕으로 정의되었었는데요,

single training example에서 얼마나 잘 작동하는지 여부를 측정합니다.

비용함수를 정의해볼텐데요,

이것은 전체적인 트레이닝세트에서 얼마나 잘 작동하는지 여부를 측정해 줄 것입니다.

비용함수 J는

W 파라미터에 적용될텐데요, B 파라미터는 각각의 트레이닝 샘플에 적용된 m개의 loss 함수의 합과

함께 평균치가 될 것입니다.

그리고 여기 ŷ은 당연히,

로지스텍 회귀분석법 알고리즘에 의거한 결과값의 예상치입니다,

W와 B 파라미터세트를 사용해서 나온 값 말이죠.

부가 설명을 드리자면,

이 것은 1 나누기

m이 비용함수가 1에서 m까지의 합인 경우입니다.

그러므로 이값은 y (i) Log ŷ

(i) 더하기 1 선을 적용하면 y (i) log one 선을 적용하여 ŷ (i)가 됩니다..

여기에 대괄호를 기입할 수도 있겠죠.

여기 마이너스 부호는 밖에 위치시킵니다.

여기서는 이런 표현를 쓸텐데요.

loss 함수가 single training example에 적용되었다고 말이죠.

비용함수는 파라미터의 비용을 나타낸 것이구요,

그러므로 로지스틱 회귀분석법 모델을 트레이닝 시키는데 있어,

W와 B라는 파라미터를 찾고

밑에 있는 전체적인 기계의 비용 J를 줄일 것입니다.

이제까지 로지스틱 회귀분석법 알고리즘의 세팅,

트레이닝 example의 비용함수,

그리고 알고리즘의 전체 비용함수의 파라미터

알고보니 로지스틱 회귀분석법은 아주 아주 작은 신경망 네트워크로 볼 수 있겠습니다.

다음 비디오에서는 이 내용에 대해 다룰텐데요

신경망의 직관적인 부분은 미리 생각해 놓으실 수 있습니다.

자 그럼 다름 비디오로 넘어가서

어떻게 로지스틱 회귀분석법을 아주 작은 신경망으로 볼 수 있는지 알아보겠습니다.


여기서 두가지 유사한 개념이 나온다. 
Loss fuction = $L(\hat{y}, h) = 

여기서 두가지 개념이 나오는데, Loss function과 cost function 두 개념이 나온다. 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE2MzU4MzI2MzcsLTQ3OTU1NTcwNCwtMj
AyNTAzNzIwOCwxMTA2NzIyNTE5LDkwODc1ODA0MiwtMTM3MzU4
MDI1XX0=
-->